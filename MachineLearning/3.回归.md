回归(Regression)任务需要输出一个数值(Scalar)。比如

- 预测股价

![](./Images/3/1.png)

- 自动驾驶车

  ![](./Images/3/2.png)

- 推荐系统

  ![](./Images/3/3.png)

但我们并不想一开始就机器学习方法进行这么复杂的任务谈起，我们可以尝试做一个稍微有趣一些的任务——预测宝可梦进化后的CP(Combat Power，攻击力)值。

![](./Images/3/4.png)

这里用
<img src="https://www.zhihu.com/equation?tex=x" alt="x" class="ee_img tr_noresize" eeimg="1">
表示宝可梦的属性值，
<img src="https://www.zhihu.com/equation?tex=y" alt="y" class="ee_img tr_noresize" eeimg="1">
表示标签值。具体来说，
<img src="https://www.zhihu.com/equation?tex=x_{cp}" alt="x_{cp}" class="ee_img tr_noresize" eeimg="1">
表示进化前的CP值，
<img src="https://www.zhihu.com/equation?tex=x_w" alt="x_w" class="ee_img tr_noresize" eeimg="1">
表示体重，
<img src="https://www.zhihu.com/equation?tex=x_h" alt="x_h" class="ee_img tr_noresize" eeimg="1">
表示高度，
<img src="https://www.zhihu.com/equation?tex=x_s" alt="x_s" class="ee_img tr_noresize" eeimg="1">
表示种类，
<img src="https://www.zhihu.com/equation?tex=x_{hp}" alt="x_{hp}" class="ee_img tr_noresize" eeimg="1">
表示生命值。

## 第一步：设计模型(Model)

按照之前所学，机器学习第一步是定义一系列函数(设计模型)。比如我们可以将模型定为：
<img src="https://www.zhihu.com/equation?tex=y=b+w·x_{cp}" alt="y=b+w·x_{cp}" class="ee_img tr_noresize" eeimg="1">
，它蕴含着无限个函数，比如
<img src="https://www.zhihu.com/equation?tex=f_1: y=10.0+9.0·x_{cp}" alt="f_1: y=10.0+9.0·x_{cp}" class="ee_img tr_noresize" eeimg="1">
，
<img src="https://www.zhihu.com/equation?tex=f_2:y=9.8+9.2·x_{cp}" alt="f_2:y=9.8+9.2·x_{cp}" class="ee_img tr_noresize" eeimg="1">
等等。

![](./Images/3/5.png)

像
<img src="https://www.zhihu.com/equation?tex=y=b+\sum{w_i·x_i}" alt="y=b+\sum{w_i·x_i}" class="ee_img tr_noresize" eeimg="1">
这类模型被称为线性模型(Linear model)，其中
<img src="https://www.zhihu.com/equation?tex=x_i" alt="x_i" class="ee_img tr_noresize" eeimg="1">
代表属性，
<img src="https://www.zhihu.com/equation?tex=w_i" alt="w_i" class="ee_img tr_noresize" eeimg="1">
代表权值，
<img src="https://www.zhihu.com/equation?tex=b" alt="b" class="ee_img tr_noresize" eeimg="1">
称为偏置。

##第二步：定义函数好坏(Goodness of Function)

为了之后表达方便我们引进两个新符号，
<img src="https://www.zhihu.com/equation?tex=x^n" alt="x^n" class="ee_img tr_noresize" eeimg="1">
表示第
<img src="https://www.zhihu.com/equation?tex=n" alt="n" class="ee_img tr_noresize" eeimg="1">
个样例的属性值，
<img src="https://www.zhihu.com/equation?tex=\hat{y}^n" alt="\hat{y}^n" class="ee_img tr_noresize" eeimg="1">
表示第
<img src="https://www.zhihu.com/equation?tex=n" alt="n" class="ee_img tr_noresize" eeimg="1">
个样例的标签值。

![](./Images/3/6.png)

假设我们有10个样例数据，

![](./Images/3/7.png)

对于定义的模型下的函数，我们可以构造一个损失函数
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
以表达函数预测值
<img src="https://www.zhihu.com/equation?tex=y" alt="y" class="ee_img tr_noresize" eeimg="1">
与真实标签值
<img src="https://www.zhihu.com/equation?tex=\hat{y}" alt="\hat{y}" class="ee_img tr_noresize" eeimg="1">
之间的误差。

![](./Images/3/9.png)

如上图所示，这里我们使用的损失函数为

<img src="https://www.zhihu.com/equation?tex=L(f)=\sum_{n=1}^{10}{\left ( \hat{y}^n-f\left ( x_{cp}^{n} \right ) \right )^2}" alt="L(f)=\sum_{n=1}^{10}{\left ( \hat{y}^n-f\left ( x_{cp}^{n} \right ) \right )^2}" class="ee_img tr_noresize" eeimg="1">

即

<img src="https://www.zhihu.com/equation?tex=L(f)=\sum_{n=1}^{10}{\left ( \hat{y}^n-\left ( b + w ·x_{cp}^{n} \right ) \right )^2}" alt="L(f)=\sum_{n=1}^{10}{\left ( \hat{y}^n-\left ( b + w ·x_{cp}^{n} \right ) \right )^2}" class="ee_img tr_noresize" eeimg="1">

对于不同的
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
和
<img src="https://www.zhihu.com/equation?tex=b" alt="b" class="ee_img tr_noresize" eeimg="1">
，将10个样例数据代入
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
得到不同的损失值。

![](./Images/3/10.png)

我们可以枚举
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
和
<img src="https://www.zhihu.com/equation?tex=b" alt="b" class="ee_img tr_noresize" eeimg="1">
的可能取值，然后找到能使损失函数
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
取到最小取值的
<img src="https://www.zhihu.com/equation?tex=w^*" alt="w^*" class="ee_img tr_noresize" eeimg="1">
和
<img src="https://www.zhihu.com/equation?tex=b^*" alt="b^*" class="ee_img tr_noresize" eeimg="1">
，这样便可以确定模型中能使损失函数
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
取到最小值的最好函数
<img src="https://www.zhihu.com/equation?tex=f^*" alt="f^*" class="ee_img tr_noresize" eeimg="1">
。但这种枚举的方法显然是很没有效率的，我们可以用更加高效的梯度下降(Gradient Descent)方法找到最好函数
<img src="https://www.zhihu.com/equation?tex=f^*" alt="f^*" class="ee_img tr_noresize" eeimg="1">
。

## 第三步：选出最好的函数(Best Function)

在学习梯度下降方法之前，先捋一捋思路，我们的目标是找到
<img src="https://www.zhihu.com/equation?tex=w^*" alt="w^*" class="ee_img tr_noresize" eeimg="1">
能使损失函数
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
取到最小值的最好函数
<img src="https://www.zhihu.com/equation?tex=f^*" alt="f^*" class="ee_img tr_noresize" eeimg="1">
。

![](./Images/3/12.png)

其实就是找到
<img src="https://www.zhihu.com/equation?tex=f^*" alt="f^*" class="ee_img tr_noresize" eeimg="1">
对应的
<img src="https://www.zhihu.com/equation?tex=w^*" alt="w^*" class="ee_img tr_noresize" eeimg="1">
和
<img src="https://www.zhihu.com/equation?tex=b^*" alt="b^*" class="ee_img tr_noresize" eeimg="1">
。

### 梯度下降(Gradient Descent)

我们先考虑模型只有一个参数
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
的情况，

![](./Images/3/13.png)

首先，随机选一个
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
的初始值
<img src="https://www.zhihu.com/equation?tex=w^0" alt="w^0" class="ee_img tr_noresize" eeimg="1">
，然后计算
<img src="https://www.zhihu.com/equation?tex=\frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w^0}" alt="\frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w^0}" class="ee_img tr_noresize" eeimg="1">
，若该值为负数，则要让
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
增大以减少
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
的值；若该值为正数，则要让
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
减小以减少
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
的值。由此，我们可以得到如下参数
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
的更新方式：

![](./Images/3/14.png)

这里的
<img src="https://www.zhihu.com/equation?tex=\frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w^0}" alt="\frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w^0}" class="ee_img tr_noresize" eeimg="1">
可以这么理解：它表示损失函数
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
在
<img src="https://www.zhihu.com/equation?tex=w^0" alt="w^0" class="ee_img tr_noresize" eeimg="1">
处的变化率，即单位
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
能让
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
的变化量。
<img src="https://www.zhihu.com/equation?tex=\eta" alt="\eta" class="ee_img tr_noresize" eeimg="1">
称为学习速率(Learning Rate)。
<img src="https://www.zhihu.com/equation?tex=\eta \frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w^0}" alt="\eta \frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w^0}" class="ee_img tr_noresize" eeimg="1">
就表示
<img src="https://www.zhihu.com/equation?tex=\eta" alt="\eta" class="ee_img tr_noresize" eeimg="1">
个单位
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
让
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
的变化量，前面加一个负号就可以不用再考虑
<img src="https://www.zhihu.com/equation?tex=\frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w^0}" alt="\frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w^0}" class="ee_img tr_noresize" eeimg="1">
的正负。

![](./Images/3/15.png)

经过上图所示的多次迭代，就可以得到能使
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
取到全局最小或局部最小的
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
值。多个参数的过程是类似的，

![](./Images/3/16.png)

其中

![](./Images/3/19.png)

梯度(Gradient)：

![](./Images/3/17.png)

也就是梯度下降方法中梯度一词的由来。

那么经过多次迭代，损失函数
<img src="https://www.zhihu.com/equation?tex=L" alt="L" class="ee_img tr_noresize" eeimg="1">
一定会取到全局最小值吗？不一定，比如出现下述情况：

![](./Images/3/18.png)

我们这里处理的损失函数是凸函数，它经过多次迭代后一定能取到全局最小值。

## 结果分析

通过使用梯度下降方法，我们得到了该模型下最好的函数：

![](./Images/3/20.png)

平均训练误差为31.9，但我们更加关心的是在测试集上的误差，因为它反映了模型的泛化能力。于是我们收集另外10个宝可梦作为测试集，得到的结果如下：

![](./Images/3/21.png)

测试集上的平均误差比训练集上的误差大，那么问题来了，我们能做得更好吗？

于是设计另一个模型——增加二次项，也用梯度下降方法得到模型下最好的函数。

![](./Images/3/22.png)

效果好一些。可以做得更好吗？我们给模型再增加三次项。

![](./Images/3/23.png)

稍微好一些。那么更加复杂的模型呢？

![](./Images/3/24.png)

结果变糟了，但我们继续给模型增加五次项。

![](./Images/3/25.png)

测试集上的平均误差“炸了”，远大于之前模型得到的测试集平均误差。

### 模型选择(Model Selection)

将训练集上的平均误差对比一下就可以发现，如果我们真的能找到每个模型下最好的函数，那么越复杂的模型下它的训练平均误差会越小。为什么会这样呢？下图可以给出一个解释，

![](./Images/3/26.png)

复杂模型的包含简单模型的函数集，比如将二次模型的二次项权重置为0，它就等同于一次模型。如果我们真的能找到每个模型下最好的函数，复杂模型下的最好函数的训练误差不一定会比简单模型下的的最好函数的训练误差小，但一定不会更大。那么对于上述的五个模型，我们应该选择哪个呢？答案是三次模型。

![](./Images/3/27.png)

复杂模型在训练集上的表现会更好一些，但在测试集上表现越不会如此。可以看到，三次之后的模型的测试集平均误差骤增，远大于训练集平均误差，这就出现所谓的过拟合(Overfitting)现象。对应了那句“你把简单问题想复杂了”。我们收集更多的数据，

![](./Images/3/28.png)

似乎有别的因素影响宝可梦进化后的CP值，是什么呢？——物种。

![](./Images/3/29.png)

这个属性是我们之前的模型没有考虑到的，既然它对CP值有影响，所以我们有必要重新设计模型。

![](./Images/3/30.png)

设计了上述新模型后，我们可以用梯度下降方法找到最好的函数。它在训练集和测试集上的表现如下：

![](./Images/3/32.png)

比之前的模型效果更好。考虑到宝可梦还有很多其它的属性，比如体重，血量，高度等等，我们可以设计更加复杂的模型：

![](./Images/3/33.png)

模型过拟合了。有什么办法应对模型过拟合问题吗？当然有，比如说——正则化(Regularization)。

### 正则化(Regularization)

可以给之前的损失函数增加一项
<img src="https://www.zhihu.com/equation?tex=\lambda\sum{(w_i)^2}" alt="\lambda\sum{(w_i)^2}" class="ee_img tr_noresize" eeimg="1">
，称为正则项。

![](./Images/3/34.png)

加入正则项是因为想让模型下的函数变得更加平滑一些，函数变得平滑的话，当从未见过的测试集上的
<img src="https://www.zhihu.com/equation?tex=x" alt="x" class="ee_img tr_noresize" eeimg="1">
输入到函数中时，函数值不会应该数据集的变化而产生很大的变化，从而在测试集上的平均误差就可以有效地减少一些。其中，我们并没有将参数
<img src="https://www.zhihu.com/equation?tex=b" alt="b" class="ee_img tr_noresize" eeimg="1">
加到正则项中，因为它对函数的平滑程度没有影响。

那么如何让函数变得平滑呢？

答案是让系数
<img src="https://www.zhihu.com/equation?tex=\lambda" alt="\lambda" class="ee_img tr_noresize" eeimg="1">
增大。当
<img src="https://www.zhihu.com/equation?tex=\lambda" alt="\lambda" class="ee_img tr_noresize" eeimg="1">
越大，就意味着我们越发需要考虑正则项对整体损失函数的影响，为了让整体损失函数值减小，权值
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
应该相应减小。权值
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
越小，函数自然变得更加平滑，从而达到目的。

![](./Images/3/35.png)


<img src="https://www.zhihu.com/equation?tex=\lambda" alt="\lambda" class="ee_img tr_noresize" eeimg="1">
可以控制函数的平滑程度，但我们也不希望它太大，而是要选到合适值。如果太大，函数会变得很“佛系”(比如
<img src="https://www.zhihu.com/equation?tex=w" alt="w" class="ee_img tr_noresize" eeimg="1">
的值都为0，函数变成
<img src="https://www.zhihu.com/equation?tex=y=b" alt="y=b" class="ee_img tr_noresize" eeimg="1">
)，不管
<img src="https://www.zhihu.com/equation?tex=x" alt="x" class="ee_img tr_noresize" eeimg="1">
怎么变，我就这样了，也就是对
<img src="https://www.zhihu.com/equation?tex=x" alt="x" class="ee_img tr_noresize" eeimg="1">
太不敏感，这就会造成损失函数在训练集和测试集上的损失值变大。

## 示例

这里做一个简单的示例，我们将模型设计为：

<img src="https://www.zhihu.com/equation?tex=y=b+w·x_{cp}" alt="y=b+w·x_{cp}" class="ee_img tr_noresize" eeimg="1">

误差函数为：

<img src="https://www.zhihu.com/equation?tex=L(w,b)=\frac{1}{N}\sum_{n=1}^{N}{\left ( \hat{y}^n-\left ( b + w ·x_{cp}^{n} \right ) \right )^2}" alt="L(w,b)=\frac{1}{N}\sum_{n=1}^{N}{\left ( \hat{y}^n-\left ( b + w ·x_{cp}^{n} \right ) \right )^2}" class="ee_img tr_noresize" eeimg="1">

数据可以在[此处下载](https://www.openintro.org/book/statdata/index.php?data=pokemon)。

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("pokemon.csv")
data = df[['cp', 'cp_new']].values
test_size = 0.33
epsilon = 0.001


def compute_error(b, w, points):
    """
    计算平均损失
    """
    x = points[:, 0]
    y = points[:, 1]
    return np.mean((y - (w * x + b))**2)


def step_gradient(b, w, points, learning_rate):
    """
    计算新的w和b的值
    """
    x = points[:, 0]
    y = points[:, 1]
    error = y - (w * x + b)
    b_gradient = 2 * np.mean(error)
    w_gradient = 2 * np.mean(error * (-x))
    b_new = b - (learning_rate * b_gradient)
    w_new = w - (learning_rate * w_gradient)
    return b_new, w_new


def main():
    train_data = data[:-int(test_size * len(data))]
    test_data = data[-int(test_size * len(data)):]
    plt.figure(figsize=(20, 10))
    plt.axis([0, 500, 0, 780])
    plt.ylabel("cp_new")
    plt.xlabel("cp")
    plt.scatter(train_data[:, 0], train_data[:, 1], label="train")
    plt.scatter(test_data[:, 0], test_data[:, 1], label='test')
    plt.legend()

    w = 0.0
    b = 0.0
    learning_rate = 0.00001
    x = np.arange(500)
    y = w * x + b
    for i in range(200):
        np.random.shuffle(data)
        train_data_ = data[:-int(0.1 * len(data))]
        test_data_ = data[-int(0.1 * len(data)):]
        train_error = compute_error(b, w, train_data_)
        test_error = compute_error(b, w, test_data_)
        print("{}. 训练平均误差:".format(i + 1), train_error, "测试平均误差:", test_error)
        if (train_error > epsilon):
            y = w * x + b
            b, w = step_gradient(b, w, train_data_, learning_rate)
            print("{}. w:{} b:{}".format(i + 1, w, b))
    print("测试平均误差:", compute_error(b, w, test_data))
    plt.plot(x, y)
    plt.show()


if __name__ == "__main__":
    main()
```

训练结果如下：

![](./Images/3/36.png)

误差为：

![](./Images/3/37.png)

从结果上来看，模型还存在一些问题，但这个例子只是为了做个演示。

当然，[`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)也是方便好用的。

## 总结

本节我们进一步了解到机器的三个步骤，学习了机器学习中为解决回归这一任务的一种方法——线性模型。为了找到模型中最好的函数，我们使用了一种名为梯度下降的方法。然而设计的模型并不能万能的，它可能会出现各种问题，比如过拟合，我们可以尝试用正则化的技术解决它。